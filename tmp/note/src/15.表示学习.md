# 15. 表示学习

- **良好的信息表示可以简化后续学习任务/信息处理，表示的选择通常取决于后续的学习任务。**
  - 例如，监督学习中接近顶层的隐藏层，其表示应能够更加容易的完成训练任务。
- **表示学习提供了进行无监督学习和半监督学习(unsupervised and semi-supervised learning)的一种方法。**
  - 深度学习训练中，高质量标注数据比较珍贵，我们通常有大量的未标注数据和相对较少的标注数据。
  - 假设：**未标注数据可以学习出良好的表示** -> 无/半监督学习的可应用性

## 15.1 贪心逐层无监督预训练(Greedy Layer-Wise Unsupervised Pretraining)

- **贪心算法(greedy algorithm)**：通过每一步都做出**局部最优**选择，**期望**最终获得**全局最优解**。
  - 局部最优、不可回退、不一定能得到全局最优解
- 表示的**可迁移性**：无监督学习获取的表示，有助于开展具有相同输入域的监督学习
  - **监督学习**：可利用预训练得到的顶层特征，训练一个简单分类器；或对预训练得到的网络进行监督微调
- 应用：
  - 贪心学习过程可为多层联合训练过程寻找好的初始值，有助于训练**多层神经网络**

### 15.1.1 (贪心)无监督预训练为何有效？

#### ！Idea

##### 1. 初始参数的选择可以显著影响正则化效果

##### 2. 学习输入分布有助于学习从输入到输出的映射

#### 应用场景

- **初始表示较差时**
  - 例：one-hot向量：高维、稀疏、彼此正交的向量，所有不同的独热向量L2距离均相等，难以捕获词的相似性
  - 通过无监督预训练(如[Word2Vec](https://www.khoury.northeastern.edu/home/vip/teach/DMcourse/4_TF_supervised/notes_slides/1301.3781.pdf)和[GloVe](https://aclanthology.org/D14-1162.pdf))，模型能够学习到词嵌入(word-embeddings)，将单词映射到一个连续的向量空间，通过L2距离、余弦相似度(cosine similarity)等表示语义相似度
- **标注样本数量有限时**
- **目标函数较复杂时**
- **改进优化分类器、降低测试集误差**
  - 神经网络训练的**非确定性**：
    - 梯度接近零的点：局部最优
    - 早停(预防过拟合)、但未达到最优解
    - 梯度过大但难以下降([Hessian矩阵](https://en.wikipedia.org/wiki/Hessian_matrix)的病态条件)
  - 无监督预训练获得的参数：收敛到更小的参数空间，减小方差，更加稳定，降低过拟合风险
  - > 相关文献：[Erhan et al: Why Does Unsupervised Pre-training Help Deep Learning?](https://proceedings.mlr.press/v9/erhan10a/erhan10a.pdf)

#### 不足

1. 双阶段训练(无监督+有监督)：更加复杂、耗时
2. 超参数设置复杂、反馈延迟
3. 无法灵活调整正则化强度
4. 缺乏对成熟监督学习方法的竞争力

！此处，可通过中等数据集MNIST对比监督学习和无监督学习的效果(结合d2l)。
！注意，PR前删掉data文件夹，这是跑MNIST时自动下载的

## 15.2 迁移学习和领域自适应

