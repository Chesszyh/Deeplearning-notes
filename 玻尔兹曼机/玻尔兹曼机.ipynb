{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 玻尔兹曼机\n",
    "\n",
    "玻尔兹曼机是一种基于能量的模型, 它的联合概率分布函数为\n",
    "$$P(x) = \\frac{\\exp(-E(x))}{Z}$$\n",
    "\n",
    "其中 $E(x)$ 是能量函数，$Z$ 是确保 $\\sum_x P(x) = 1$ 的配分函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 受限玻尔兹曼机（RBM）\n",
    "受限玻尔兹曼机其联合概率分布:\n",
    "$$\n",
    "P(v = \\mathbf{v}, h = \\mathbf{h}) = \\frac{1}{Z} \\exp(-E(\\mathbf{v}, \\mathbf{h})).\n",
    "$$\n",
    "\n",
    "受限玻尔兹曼机的能量函数：\n",
    "\n",
    "$$\n",
    "E(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{b}^\\top \\mathbf{v} - \\mathbf{c}^\\top \\mathbf{h} - \\mathbf{v}^\\top W \\mathbf{h},\n",
    "$$\n",
    "\n",
    "其中 $Z$ 是被称为配分函数的归一化常数：\n",
    "\n",
    "$$\n",
    "Z = \\sum_{\\mathbf{v}} \\sum_{\\mathbf{h}} \\exp\\{-E(\\mathbf{v}, \\mathbf{h})\\}\n",
    "$$\n",
    "\n",
    "下图为RBM的简单示意图：受限玻尔兹曼机是基于二分图的无向图模型\n",
    "\n",
    "<img src=\"RBM示意图.png\"  width=\"380\" height=\"230\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBM代码实现\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义RBM类\n",
    "\n",
    "class RBM(nn.Module):\n",
    "    def __init__(self, n_vis, n_hin):\n",
    "        super(RBM, self).__init__()\n",
    "        # n_vis: 可见层单元数\n",
    "        # n_hin: 隐藏层单元数\n",
    "        self.n_vis = n_vis\n",
    "        self.n_hin = n_hin\n",
    "\n",
    "        # 权重矩阵，初始化为正态分布\n",
    "        self.W = nn.Parameter(torch.randn(n_hin, n_vis) * 1e-2)\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_vis))\n",
    "        self.h_bias = nn.Parameter(torch.zeros(n_hin))\n",
    "\n",
    "    def sample_h(self, v):\n",
    "        # 计算隐藏层的概率\n",
    "        wx = torch.matmul(v, self.W.t())\n",
    "        activation = wx + self.h_bias\n",
    "        p_h_given_v = torch.sigmoid(activation)\n",
    "        return p_h_given_v, torch.bernoulli(p_h_given_v)\n",
    "\n",
    "    def sample_v(self, h):\n",
    "        # 计算可见层的概率\n",
    "        wx = torch.matmul(h, self.W)\n",
    "        activation = wx + self.v_bias\n",
    "        p_v_given_h = torch.sigmoid(activation)\n",
    "        return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
    "\n",
    "    def forward(self, v):\n",
    "        # 单次 Gibbs 采样\n",
    "        p_h, h = self.sample_h(v)\n",
    "        p_v, v = self.sample_v(h)\n",
    "        return v\n",
    "\n",
    "    def contrastive_divergence(self, input_data, lr=0.01, k=1):\n",
    "        # 正向传播\n",
    "        p_h_positive, h_positive = self.sample_h(input_data)\n",
    "\n",
    "        # 负向传播\n",
    "        v_k = input_data\n",
    "        for _ in range(k):\n",
    "            p_h, h = self.sample_h(v_k)\n",
    "            p_v, v_k = self.sample_v(h)\n",
    "\n",
    "        p_h_negative, _ = self.sample_h(v_k)\n",
    "\n",
    "        # 计算梯度\n",
    "        positive_grad = torch.matmul(p_h_positive.t(), input_data)\n",
    "        negative_grad = torch.matmul(p_h_negative.t(), v_k)\n",
    "\n",
    "        # 更新权重和偏置\n",
    "        self.W.data += lr * (positive_grad - negative_grad) / input_data.size(0)\n",
    "        self.v_bias.data += lr * torch.mean(input_data - v_k, dim=0)\n",
    "        self.h_bias.data += lr * torch.mean(p_h_positive - p_h_negative, dim=0)\n",
    "\n",
    "        # 计算重构误差\n",
    "        loss = torch.mean((input_data - v_k) ** 2)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度信念网络（DBN）\n",
    "\n",
    "深度信念网络是具有若干潜变量层的生成模型。潜变量通常是二值的，而可见单元可以是二值或实数， DBN概率分布为下列公式\n",
    "\n",
    "$$\n",
    "P(h^{(l)}, h^{(l-1)}) \\propto \\exp \\left( b^{(l)^\\top} h^{(l)} + b^{(l-1)^\\top} h^{(l-1)} + h^{(l-1)^\\top} W^{(l)} h^{(l)} \\right),\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(h^{(k)}_i = 1 \\mid h^{(k+1)}) = \\sigma \\left( b^{(k)}_i + W^{(k+1)^\\top}_{:, i} h^{(k+1)} \\right) \\quad \\forall i, \\forall k \\in 1, \\dots, l-2,\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(v_i = 1 \\mid h^{(1)}) = \\sigma \\left( b^{(0)}_i + W^{(1)^\\top}_{:, i} h^{(1)} \\right) \\quad \\forall i.\n",
    "$$\n",
    "\n",
    "已下为DBN示意图：深度信念网络是涉及有向和无向连接的混合图模型\n",
    "\n",
    "<img src=\"DBN示意图.png\"  width=\"380\" height=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining RBM layer 1/2\n",
      "Epoch 1, Loss: 0.147926887914316\n",
      "Epoch 2, Loss: 0.10845467882719376\n",
      "Epoch 3, Loss: 0.09534132804697765\n",
      "Epoch 4, Loss: 0.08743825155312318\n",
      "Epoch 5, Loss: 0.0820729368801183\n",
      "Pretraining RBM layer 2/2\n",
      "Epoch 1, Loss: 0.3485538379342825\n",
      "Epoch 2, Loss: 0.28871215371561965\n",
      "Epoch 3, Loss: 0.27034404956455677\n",
      "Epoch 4, Loss: 0.259960687522695\n",
      "Epoch 5, Loss: 0.25401915649551826\n",
      "Pretraining completed.\n"
     ]
    }
   ],
   "source": [
    "# DBN代码\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 定义DBN类\n",
    "class DBN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DBN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(RBM(layers[i], layers[i+1]))\n",
    "\n",
    "    def pretrain(self, train_loader, lr=0.01, k=1, epochs=10):\n",
    "        input_data = None\n",
    "        for i, rbm in enumerate(self.layers):\n",
    "            print(f\"Pretraining RBM layer {i+1}/{len(self.layers)}\")\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for batch, _ in train_loader:\n",
    "                    batch = batch.view(batch.size(0), -1)\n",
    "                    if input_data is not None:\n",
    "                        with torch.no_grad():\n",
    "                            batch = input_data(batch)\n",
    "                    # 确保概率值在 [0,1] 之间\n",
    "                    batch = torch.clamp(batch, 0, 1)\n",
    "                    batch = batch.bernoulli()  # 二值化\n",
    "                    loss = rbm.contrastive_divergence(batch, lr=lr, k=k)\n",
    "                    epoch_loss += loss.item()\n",
    "                print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "            # 设置下一层的输入为当前 RBM 的隐藏层输出\n",
    "            input_data = lambda x, rbm=rbm: torch.sigmoid(torch.matmul(x, rbm.W.t()) + rbm.h_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for rbm in self.layers:\n",
    "            p_h, h = rbm.sample_h(x)\n",
    "            x = h\n",
    "        return x\n",
    "\n",
    "def train_dbn():\n",
    "    batch_size = 64\n",
    "    lr = 0.01\n",
    "    k = 1\n",
    "    epochs = 5\n",
    "    layers = [784, 500, 200]  # 输入层和两层隐藏层\n",
    "\n",
    "    # 数据预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 初始化 DBN\n",
    "    dbn = DBN(layers)\n",
    "\n",
    "    # 预训练\n",
    "    dbn.pretrain(train_loader, lr=lr, k=k, epochs=epochs)\n",
    "    print(\"Pretraining completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dbn()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度玻尔兹曼机（DBM）\n",
    "\n",
    "深度玻尔兹曼机包含一个可见层 $v$ 和三个隐藏层 $h^{(1)}, h^{(2)}$ 和 $h^{(3)}$ 的情况下，联合概率为下面公式：\n",
    "\n",
    "$$\n",
    "P(v, h^{(1)}, h^{(2)}, h^{(3)}) = \\frac{1}{Z(\\theta)} \\exp \\left( -E(v, h^{(1)}, h^{(2)}, h^{(3)}; \\theta) \\right).\n",
    "$$\n",
    "\n",
    "深度玻尔兹曼机的能量函数为：\n",
    "$$\n",
    "E(v, h^{(1)}, h^{(2)}, h^{(3)}; \\theta) = -v^\\top W^{(1)} h^{(1)} - h^{(1)\\top} W^{(2)} h^{(2)} - h^{(2)\\top} W^{(3)} h^{(3)}.\n",
    "$$\n",
    "\n",
    "下图为深度玻尔兹曼机示意图：深度玻尔兹曼机是具有几层潜变量的无向图模型\n",
    "\n",
    "<img src=\"DBM示意图.png\"  width=\"380\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining RBM layer 1/2\n",
      "Epoch 1, Loss: 0.14769503898394387\n",
      "Epoch 2, Loss: 0.1083591301510456\n",
      "Epoch 3, Loss: 0.0952612899783959\n",
      "Epoch 4, Loss: 0.087439653795284\n",
      "Epoch 5, Loss: 0.08202889150203165\n",
      "Pretraining RBM layer 2/2\n",
      "Epoch 1, Loss: 0.348730943763434\n",
      "Epoch 2, Loss: 0.289149253511988\n",
      "Epoch 3, Loss: 0.27082059551467264\n",
      "Epoch 4, Loss: 0.2607529981812434\n",
      "Epoch 5, Loss: 0.25463259614098555\n",
      "Pretraining completed.\n",
      "Pretraining completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DBM代码\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 定义DBM类\n",
    "class DBM(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DBM, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(RBM(layers[i], layers[i+1]))\n",
    "        self.n_layers = len(self.layers)\n",
    "\n",
    "    def pretrain(self, train_loader, lr=0.01, k=1, epochs=5):\n",
    "        # 逐层预训练每个RBM\n",
    "        input_data = None\n",
    "        for i, rbm in enumerate(self.layers):\n",
    "            print(f\"Pretraining RBM layer {i+1}/{self.n_layers}\")\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for batch, _ in train_loader:\n",
    "                    batch = batch.view(batch.size(0), -1)\n",
    "                    if input_data is not None:\n",
    "                        with torch.no_grad():\n",
    "                            batch = input_data(batch)\n",
    "                    batch = torch.clamp(batch, 0, 1)\n",
    "                    batch = batch.bernoulli()  # 二值化\n",
    "                    loss = rbm.contrastive_divergence(batch, lr=lr, k=k)\n",
    "                    epoch_loss += loss.item()\n",
    "                print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "            # 设置下一层的输入为当前 RBM 的隐藏层输出\n",
    "            def new_input(x, rbm=rbm):\n",
    "                p_h, h = rbm.sample_h(x)\n",
    "                return p_h\n",
    "            input_data = new_input\n",
    "\n",
    "    def forward(self, x):\n",
    "        for rbm in self.layers:\n",
    "            p_h, h = rbm.sample_h(x)\n",
    "            x = h\n",
    "        return x\n",
    "\n",
    "    def gibbs_sampling(self, v0, steps=1):\n",
    "        # 在DBM中执行多步Gibbs采样\n",
    "        vk = v0\n",
    "        for step in range(steps):\n",
    "            for rbm in self.layers:\n",
    "                p_h, h = rbm.sample_h(vk)\n",
    "                p_v, vk = rbm.sample_v(h)\n",
    "        return vk\n",
    "\n",
    "def train_dbm():\n",
    "    batch_size = 64\n",
    "    lr = 0.01\n",
    "    k = 1\n",
    "    epochs = 5\n",
    "    layers = [784, 500, 200]\n",
    "\n",
    "    # 数据预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 初始化 DBM\n",
    "    dbm = DBM(layers)\n",
    "\n",
    "    # 预训练\n",
    "    dbm.pretrain(train_loader, lr=lr, k=k, epochs=epochs)\n",
    "    print(\"Pretraining completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dbm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实值数据上的玻尔兹曼机\n",
    "### Gaussian-Bernoulli RBM\n",
    "\n",
    "Gaussian-Bernoulli RBM 上定义能量函数：\n",
    "$$\n",
    "E(v, h) = \\frac{1}{2} v^\\top (\\beta \\odot v) - (v \\odot \\beta)^\\top W h - b^\\top h,\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class GRBM(nn.Module):\n",
    "    def __init__(self, n_vis, n_hin, sigma=1.0):\n",
    "        super(GRBM, self).__init__()\n",
    "        # n_vis: 可见层单元数\n",
    "        # n_hin: 隐藏层单元数\n",
    "        # sigma: 高斯分布的标准差\n",
    "        self.n_vis = n_vis\n",
    "        self.n_hin = n_hin\n",
    "        self.sigma = sigma  # 标准差\n",
    "\n",
    "        # 权重矩阵，初始化为正态分布\n",
    "        self.W = nn.Parameter(torch.randn(n_hin, n_vis) * 0.01)\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_vis))  \n",
    "        self.h_bias = nn.Parameter(torch.zeros(n_hin))  \n",
    "\n",
    "    def sample_h(self, v):\n",
    "        activation = torch.matmul(v, self.W.t()) + self.h_bias\n",
    "        p_h_given_v = torch.sigmoid(activation)\n",
    "        h_sample = torch.bernoulli(p_h_given_v)\n",
    "        return p_h_given_v, h_sample\n",
    "\n",
    "    def sample_v(self, h):\n",
    "        # 对于GRBM，可见层激活为高斯分布\n",
    "        activation = torch.matmul(h, self.W) + self.v_bias\n",
    "        p_v_given_h = activation  # 均值为线性激活值\n",
    "        v_sample = p_v_given_h + torch.randn_like(p_v_given_h) * self.sigma  # 添加高斯噪声\n",
    "        return p_v_given_h, v_sample\n",
    "\n",
    "    def forward(self, v):\n",
    "        # 单次 Gibbs 采样\n",
    "        p_h, h = self.sample_h(v)\n",
    "        p_v, v = self.sample_v(h)\n",
    "        return v\n",
    "\n",
    "    def contrastive_divergence(self, input_data, lr=0.01, k=1):\n",
    "        # 正向传播\n",
    "        p_h_positive, h_positive = self.sample_h(input_data)\n",
    "\n",
    "        # 负向传播\n",
    "        v_k = input_data\n",
    "        for _ in range(k):\n",
    "            p_h, h = self.sample_h(v_k)\n",
    "            p_v, v_k = self.sample_v(h)\n",
    "\n",
    "        # 隐藏层激活概率\n",
    "        p_h_negative, _ = self.sample_h(v_k)\n",
    "\n",
    "        # 计算梯度\n",
    "        positive_grad = torch.matmul(p_h_positive.t(), input_data)\n",
    "        negative_grad = torch.matmul(p_h_negative.t(), v_k)\n",
    "\n",
    "        # 更新权重和偏置\n",
    "        self.W.data += lr * (positive_grad - negative_grad) / input_data.size(0)\n",
    "        self.v_bias.data += lr * torch.mean(input_data - v_k, dim=0)\n",
    "        self.h_bias.data += lr * torch.mean(p_h_positive - p_h_negative, dim=0)\n",
    "\n",
    "        # 计算重构误差\n",
    "        loss = torch.mean((input_data - v_k) ** 2)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssRBM\n",
    "\n",
    "ssRBM的能量函数：\n",
    "\n",
    "$$\n",
    "E_{SS}(x, s, h) = -\\sum_i x^\\top W_{:,i} s_i h_i + \\frac{1}{2} x^\\top \\left( \\Lambda + \\sum_i \\Phi_i h_i \\right) x\n",
    "+ \\frac{1}{2} \\sum_i \\alpha_i s_i^2 - \\sum_i \\alpha_i \\mu_i s_i h_i - \\sum_i b_i h_i + \\sum_i \\alpha_i \\mu_i^2 h_i,\n",
    "$$\n",
    "\n",
    "其中 $b_i$ 是尖峰 $h_i$ 的偏置，$\\Lambda$ 是观测值 $x$ 上的对角精度矩阵。参数 $\\alpha_i > 0$ 是实值平板变量 $s_i$ 的标准精度参数。参数 $\\Phi_i$ 是定义 $x$ 上的 $h$ 调制二次惩罚的非负对角矩阵。每个 $\\mu_i$ 是平板变量 $s_i$ 的均值参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class SpikeAndSlabRBM(nn.Module):\n",
    "    def __init__(self, n_vis, n_hin, sigma=1.0):\n",
    "        super(SpikeAndSlabRBM, self).__init__()\n",
    "        self.n_vis = n_vis\n",
    "        self.n_hin = n_hin\n",
    "        self.sigma = sigma  \n",
    "\n",
    "        # 权重矩阵，初始化为正态分布\n",
    "        self.W = nn.Parameter(torch.randn(n_hin, n_vis) * 0.01)\n",
    "        # 可学习的偏置\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_vis))   # 可见层偏置\n",
    "        self.s_bias = nn.Parameter(torch.zeros(n_hin))   # 尖峰偏置\n",
    "        self.z_bias = nn.Parameter(torch.zeros(n_hin))   # 平板偏置\n",
    "\n",
    "    def sample_s(self, v):\n",
    "        activation = torch.matmul(v, self.W.t()) + self.s_bias\n",
    "        p_s = torch.sigmoid(activation)\n",
    "        s_sample = torch.bernoulli(p_s)\n",
    "        return p_s, s_sample\n",
    "\n",
    "    def sample_z(self, v, s):\n",
    "        # 平板的均值\n",
    "        mu_z = torch.matmul(v, self.W.t()) + self.z_bias\n",
    "        # 平板的采样：仅在尖峰s=1时采样，否则为0\n",
    "        mu_z = mu_z * s\n",
    "        z_sample = mu_z + torch.randn_like(mu_z) * self.sigma\n",
    "        return mu_z, z_sample\n",
    "\n",
    "    def sample_h(self, v):\n",
    "        p_s, s_sample = self.sample_s(v)\n",
    "        mu_z, z_sample = self.sample_z(v, s_sample)\n",
    "        h_sample = z_sample  # 隐藏层的最终采样结果\n",
    "        return p_s, s_sample, h_sample\n",
    "\n",
    "    def sample_v(self, h):\n",
    "        activation = torch.matmul(h, self.W) + self.v_bias\n",
    "        p_v = activation  # 可见层的均值\n",
    "        v_sample = p_v + torch.randn_like(p_v) * self.sigma  # 添加高斯噪声\n",
    "        return p_v, v_sample\n",
    "\n",
    "    def forward(self, v):\n",
    "        #单次 Gibbs 采样\n",
    "        p_s, s, h = self.sample_h(v)\n",
    "        p_v, v = self.sample_v(h)\n",
    "        return v\n",
    "\n",
    "    def contrastive_divergence(self, input_data, lr=0.01, k=1):\n",
    "        # 正向传播（Positive Phase）\n",
    "        p_s_pos, s_pos, h_pos = self.sample_h(input_data)\n",
    "\n",
    "        # 负向传播（Negative Phase）\n",
    "        v_neg = input_data\n",
    "        for _ in range(k):\n",
    "            p_s_neg, s_neg, h_neg = self.sample_h(v_neg)\n",
    "            p_v_neg, v_neg = self.sample_v(h_neg)\n",
    "\n",
    "        # 隐藏层激活概率（负向传播后）\n",
    "        p_s_neg, s_neg, h_neg = self.sample_h(v_neg)\n",
    "\n",
    "        # 计算梯度\n",
    "        positive_grad = torch.matmul(p_s_pos.t(), input_data)\n",
    "        negative_grad = torch.matmul(p_s_neg.t(), v_neg)\n",
    "\n",
    "        # 更新权重和偏置\n",
    "        self.W.data += lr * (positive_grad - negative_grad) / input_data.size(0)\n",
    "        self.v_bias.data += lr * torch.mean(input_data - v_neg, dim=0)\n",
    "        self.s_bias.data += lr * torch.mean(p_s_pos - p_s_neg, dim=0)\n",
    "        self.z_bias.data += lr * torch.mean(torch.matmul(p_s_pos.t(), input_data) - torch.matmul(p_s_neg.t(), v_neg), dim=0)\n",
    "\n",
    "        # 计算重构误差\n",
    "        loss = torch.mean((input_data - v_neg) ** 2)\n",
    "        return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
