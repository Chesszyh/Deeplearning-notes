{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 MNIST 数据集与受限玻尔兹曼机 (RBM) 演示马尔可夫链混合困难\n",
    "\n",
    "### 目标\n",
    "\n",
    "使用 MNIST 数据集训练受限玻尔兹曼机（RBM），并通过 Gibbs 采样生成样本，演示马尔可夫链采样难以  \n",
    "在不同模式（数字类别）之间混合的问题，复现图 17.2 的现象。\n",
    "\n",
    "### 背景介绍\n",
    "\n",
    "在高维数据（如图像）上，马尔可夫链蒙特卡罗方法（MCMC）中的 Gibbs 采样可能会遇到混合不良的问题。  \n",
    "具体来说，连续的采样可能会在某个模式（如特定的数字类别）上停留较长时间，难以跳转到其他模式。这是  \n",
    "因为在高维空间中，不同模式之间被低概率的高能量区域分隔，导致采样难以跨越这些区域。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现步骤\n",
    "\n",
    "1. **数据准备**：加载并预处理 MNIST 数据集。\n",
    "2. **RBM 模型定义**：使用 PyTorch 定义受限玻尔兹曼机 (RBM) 模型。\n",
    "3. **模型训练**：在 MNIST 数据集上训练 RBM 模型。\n",
    "4. **Gibbs 采样**：从训练好的 RBM 模型中使用 Gibbs 采样生成样本。\n",
    "5. **结果展示**：可视化连续的采样结果，观察混合情况。\n",
    "\n",
    "### 结论\n",
    "\n",
    "通过上述实验，可以观察到在高维数据上 Gibbs 采样的混合困难现象，特别是在不同模式（数字类别）之间  \n",
    "的转换过程中。这种现象反映了马尔可夫链蒙特卡罗方法在高维空间中面临的挑战。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据转换（移除归一化步骤）\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为张量，值范围在 [0, 1]\n",
    "    # transforms.Normalize((0.5,), (0.5,))  # 移除归一化到 [-1, 1] 的步骤\n",
    "])\n",
    "\n",
    "# 加载MNIST数据集\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module):\n",
    "    def __init__(self, n_vis, n_hid):\n",
    "        super(RBM, self).__init__()\n",
    "        self.n_vis = n_vis  # 可见层节点数（像素数）\n",
    "        self.n_hid = n_hid  # 隐层节点数\n",
    "\n",
    "        # 初始化参数\n",
    "        self.W = nn.Parameter(torch.randn(n_vis, n_hid) * 0.01)  # 权重矩阵，形状应为 (n_vis, n_hid)\n",
    "        self.h_bias = nn.Parameter(torch.zeros(n_hid))  # 隐层偏置\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_vis))  # 可见层偏置\n",
    "\n",
    "    def sample_from_p(self, p):\n",
    "        return torch.bernoulli(p)\n",
    "\n",
    "    def v_to_h(self, v):\n",
    "        p_h = torch.sigmoid(F.linear(v, self.W.t(), self.h_bias))\n",
    "        return p_h\n",
    "\n",
    "    def h_to_v(self, h):\n",
    "        p_v = torch.sigmoid(F.linear(h, self.W, self.v_bias))\n",
    "        return p_v\n",
    "\n",
    "    def forward(self, v):\n",
    "        p_h = self.v_to_h(v)\n",
    "        h = self.sample_from_p(p_h)\n",
    "        p_v = self.h_to_v(h)\n",
    "        return p_v\n",
    "\n",
    "    def free_energy(self, v):\n",
    "        vbias_term = v.mv(self.v_bias)\n",
    "        wx_b = F.linear(v, self.W.t(), self.h_bias)\n",
    "        hidden_term = torch.sum(F.softplus(wx_b), dim=1)\n",
    "        return (-vbias_term - hidden_term).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 7395874.34707557\n",
      "Epoch 2/5, Loss: 22189158.28144989\n",
      "Epoch 3/5, Loss: 36982472.08528785\n",
      "Epoch 4/5, Loss: 51776733.867803834\n",
      "Epoch 5/5, Loss: 66572545.28358209\n"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "batch_size = 64\n",
    "n_vis = 28 * 28  # 输入层节点数\n",
    "n_hid = 256      # 隐层节点数\n",
    "k = 1            # CD-k 中的 k 值\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 实例化RBM模型\n",
    "rbm = RBM(n_vis, n_hid)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(rbm.parameters(), lr=0.1)\n",
    "\n",
    "# 训练RBM\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for data, _ in train_loader:\n",
    "        data = data.view(-1, n_vis)\n",
    "        data = data.bernoulli()  # 将输入二值化\n",
    "\n",
    "        # 正向传播\n",
    "        v = data  # 可见层\n",
    "\n",
    "        # 正相阶段\n",
    "        p_h = rbm.v_to_h(v)\n",
    "        h = rbm.sample_from_p(p_h)\n",
    "\n",
    "        # 负相阶段（CD-k）\n",
    "        v_k = v\n",
    "        for _ in range(k):\n",
    "            p_h = rbm.v_to_h(v_k)\n",
    "            h = rbm.sample_from_p(p_h)\n",
    "            p_v = rbm.h_to_v(h)\n",
    "            v_k = rbm.sample_from_p(p_v)\n",
    "\n",
    "        # 计算正相和负相的梯度\n",
    "        positive_grad = torch.matmul(v.t(), rbm.v_to_h(v))\n",
    "        negative_grad = torch.matmul(v_k.t(), rbm.v_to_h(v_k))\n",
    "\n",
    "        # 更新参数\n",
    "        rbm.W.grad = (positive_grad - negative_grad) / batch_size\n",
    "        rbm.v_bias.grad = torch.sum(v - v_k, dim=0) / batch_size\n",
    "        rbm.h_bias.grad = torch.sum(rbm.v_to_h(v) - rbm.v_to_h(v_k), dim=0) / batch_size\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 计算损失（自由能差）\n",
    "        loss = rbm.free_energy(v) - rbm.free_energy(v_k)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gibbs_sampling(rbm, k, v):\n",
    "    for _ in range(k):\n",
    "        p_h = rbm.v_to_h(v)\n",
    "        h = rbm.sample_from_p(p_h)\n",
    "        p_v = rbm.h_to_v(h)\n",
    "        v = rbm.sample_from_p(p_v)\n",
    "    return v\n",
    "\n",
    "# 从随机初始化开始采样\n",
    "num_samples = 20  # 生成20个连续的样本\n",
    "k = 1             # 每次Gibbs采样的步数\n",
    "v = torch.bernoulli(torch.rand(1, n_vis))  # 随机初始化\n",
    "\n",
    "samples = []\n",
    "for i in range(num_samples):\n",
    "    v = gibbs_sampling(rbm, k, v)\n",
    "    samples.append(v.view(28, 28).detach().numpy())\n",
    "\n",
    "# 可视化连续的样本\n",
    "fig, axes = plt.subplots(4, 5, figsize=(8, 6))  # 设置为4行5列\n",
    "axes = axes.flatten()  # 将axes数组展平，方便索引\n",
    "for idx, sample in enumerate(samples):\n",
    "    axes[idx].imshow(sample, cmap='gray')\n",
    "    axes[idx].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 结果展示与分析\n",
    "\n",
    "运行上述代码后，会显示 10 个连续的采样结果。由于 Gibbs 采样的混合问题，我们可以观察到：\n",
    "\n",
    "- **连续的样本非常相似**：  \n",
    "  它们可能是同一个数字（如连续的“3”），仅在细节上有所变化。\n",
    "\n",
    "- **难以跳转到其他模式**：  \n",
    "  很少看到从一个数字类别跳转到另一个数字类别的情况。\n",
    "\n",
    "这表明马尔可夫链在当前模式（数字类别）上停留了较长时间，难以有效地在不同模式之间混合。\n",
    "\n",
    "## 总结\n",
    "\n",
    "通过在 MNIST 数据集上训练 RBM 并使用 Gibbs 采样生成样本，我们演示了马尔可夫链采样在高维数据上混合  \n",
    "困难的问题。由于不同模式（数字类别）之间被高能量障碍分隔，采样过程难以跨越这些障碍，导致连续的样本在  \n",
    "同一模式上徘徊。这与图 17.2 左侧展示的现象一致。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意事项\n",
    "\n",
    "- **RBM 的训练**：  \n",
    "  由于 RBM 的训练较为复杂，以上代码使用了对比散度（Contrastive Divergence）算法，并进行了简化。  \n",
    "  实际应用中，可能需要更复杂的训练方法和更长的训练时间。\n",
    "\n",
    "- **Gibbs 采样步数**：  \n",
    "  增加 Gibbs 采样的步数 $k$ 可以在一定程度上改善采样结果，但仍然难以解决混合问题。\n",
    "\n",
    "- **改进方法**：  \n",
    "  为了解决混合困难的问题，可以考虑使用其他采样方法（如并行温度法）、深度模型（如深度玻尔兹曼机）或生成式对抗网络（GAN）等。\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. *Neural Computation*, 14(8), 1771-1800.\n",
    "2. Bengio, Y., & Delalleau, O. (2009). Justifying and generalizing contrastive divergence. *Neural Computation*, 21(6), 1601-1621.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
